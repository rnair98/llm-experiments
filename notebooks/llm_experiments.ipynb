{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f73ef688-7a85-42c6-80be-df953cf29285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from openai import OpenAI\n",
    "from exa_py import Exa\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored  \n",
    "from pydantic import BaseModel  \n",
    "from typing import List, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03f2f66-4a9b-4242-bf7d-65fcf3ad58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMAFILE_BASE_URL = \"http://localhost:8080/v1\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "PERPLEXITY_BASE_URL = \"https://api.perplexity.ai\"\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7bd5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_APY_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") \n",
    "EXA_API_KEY = os.getenv(\"EXA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e21068-06a8-447f-b87c-d5e2808b176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GROQ_MODEL = \"llama3-70b-8192\"\n",
    "OPENAI_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7c6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query(BaseModel):\n",
    "    topic: str\n",
    "    query: str\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[Query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fce67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exa = Exa(api_key = EXA_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ece22d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tools(num:int, tool_type:str) -> list:\n",
    "    properties = {}\n",
    "    for i in range(1, num + 1):\n",
    "        key = f'{tool_type}_{i}'\n",
    "        properties[key] = {\n",
    "            'type': 'string',\n",
    "            'description': 'Search queries that would be useful for generating a report on my main topic'\n",
    "        }\n",
    "\n",
    "    custom_function = {\n",
    "        'name': 'generate_exa_search_queries',\n",
    "        'description': 'Generates Exa search queries to investigate the main topic',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': properties\n",
    "        },\n",
    "        'required': [f'{tool_type}_{i}' for i in range(1, num + 1)]\n",
    "    }\n",
    "\n",
    "    return [{\"type\": \"function\", \"function\": custom_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6bfebca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'generate_exa_search_queries',\n",
       "   'description': 'Generates Exa search queries to investigate the main topic',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'query_1': {'type': 'string',\n",
       "      'description': 'Search queries that would be useful for generating a report on my main topic'},\n",
       "     'query_2': {'type': 'string',\n",
       "      'description': 'Search queries that would be useful for generating a report on my main topic'},\n",
       "     'query_3': {'type': 'string',\n",
       "      'description': 'Search queries that would be useful for generating a report on my main topic'},\n",
       "     'query_4': {'type': 'string',\n",
       "      'description': 'Search queries that would be useful for generating a report on my main topic'},\n",
       "     'query_5': {'type': 'string',\n",
       "      'description': 'Search queries that would be useful for generating a report on my main topic'}}},\n",
       "   'required': ['query_1', 'query_2', 'query_3', 'query_4', 'query_5']}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = generate_tools(5, \"query\")\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "342cce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=None, provider=None):\n",
    "    try:\n",
    "        if provider.lower()==\"openai\":\n",
    "            print(colored(f\"Using OpenAI...\\n\", \"green\"))\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                stream=False,   \n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=tool_choice,\n",
    "            )\n",
    "        else:\n",
    "            print(colored(f\"Using Groq...\\n\", \"green\"))\n",
    "            client = OpenAI(\n",
    "                api_key=GROQ_API_KEY,\n",
    "                base_url=GROQ_BASE_URL\n",
    "            )\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                stream=False,\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea3f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_messages_and_tools(provider:str, topic:str, num_queries:int) -> list:\n",
    "    context = \"context\" if provider.lower()==\"openai\" else f\"provided schema: {json.dumps(Queries.model_json_schema(), indent=2)}\"\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": f\"You are the world's most advanced and intelligent programming and AI Research assistant that can only be queried via an API. Based on the tools and schemas provided to you and in your arsenal, you generate the most accurate and optimized JSON responses based on the {context}.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I'm going to give you a topic I want to research. I want you to generate {num_queries} interesting, diverse search queries that would be useful for generating a report on my main topic. Here is the main topic: {topic}.\"}\n",
    "    ]\n",
    "    tools = generate_tools(num_queries, \"query\")\n",
    "    tool_choice = {\"type\": \"function\", \"function\": {\"name\": tools[0]['function']['name']}}\n",
    "    return messages, tools, tool_choice\n",
    "\n",
    "def get_completion_args(provider:str, topic:str, num_queries:int) -> Dict:\n",
    "    messages, tools, tool_choice = get_messages_and_tools(provider, topic, num_queries)\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"tools\": tools,\n",
    "        \"tool_choice\": tool_choice,\n",
    "        \"provider\": provider,\n",
    "        \"model\": OPENAI_MODEL if provider.lower()==\"openai\" else GROQ_MODEL,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12ed28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing Groq...\n",
      "\u001b[0m\n",
      "{\n",
      "  \"queries\": [\n",
      "    {\n",
      "      \"topic\": \"LLM Evaluation\",\n",
      "      \"query\": \"What are the benefits and limitations of using a panel of LLM judges to evaluate the correctness of another LLM?\"\n",
      "    },\n",
      "    {\n",
      "      \"topic\": \"LLM Evaluation Metrics\",\n",
      "      \"query\": \"What metrics can be used to evaluate the correctness of an LLM, and how can a panel of LLM judges be used to improve the evaluation process?\"\n",
      "    },\n",
      "    {\n",
      "      \"topic\": \"LLM Evaluation Methods\",\n",
      "      \"query\": \"What are the different methods for evaluating the correctness of an LLM, and how does using a panel of LLM judges compare to other methods?\"\n",
      "    },\n",
      "    {\n",
      "      \"topic\": \"LLM Judge Agreement\",\n",
      "      \"query\": \"How can the agreement between a panel of LLM judges be measured and improved, and what are the implications for evaluating the correctness of another LLM?\"\n",
      "    },\n",
      "    {\n",
      "      \"topic\": \"LLM Evaluation Bias\",\n",
      "      \"query\": \"How can bias be mitigated when using a panel of LLM judges to evaluate the correctness of another LLM, and what are the potential sources of bias in this process?\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "groq_args = get_completion_args(provider=\"groq\", topic = \"Using a panel of LLM judges to evaluate the correctness of another LLM\", num_queries=5)\n",
    "groq_res = chat_completion_request(**groq_args)\n",
    "print(json.dumps(json.loads(groq_res.choices[0].message.content), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc454ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Query(topic='LLM Evaluation', query='What are the benefits and limitations of using a panel of LLM judges to evaluate the correctness of another LLM?'),\n",
       " Query(topic='LLM Evaluation Metrics', query='What metrics can be used to evaluate the correctness of an LLM, and how can a panel of LLM judges be used to improve the evaluation process?'),\n",
       " Query(topic='LLM Evaluation Methods', query='What are the different methods for evaluating the correctness of an LLM, and how does using a panel of LLM judges compare to other methods?'),\n",
       " Query(topic='LLM Judge Agreement', query='How can the agreement between a panel of LLM judges be measured and improved, and what are the implications for evaluating the correctness of another LLM?'),\n",
       " Query(topic='LLM Evaluation Bias', query='How can bias be mitigated when using a panel of LLM judges to evaluate the correctness of another LLM, and what are the potential sources of bias in this process?')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Queries.model_validate_json(groq_res.choices[0].message.content)\n",
    "model.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "280cfad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing OpenAI...\n",
      "\u001b[0m\n",
      "{\n",
      "  \"query_1\": \"advantages of using LLM judges to evaluate other LLMs\",\n",
      "  \"query_2\": \"methodologies for implementing a panel of LLM judges\",\n",
      "  \"query_3\": \"case studies on LLM judges evaluating other LLMs\",\n",
      "  \"query_4\": \"challenges in using LLMs to judge other LLMs\",\n",
      "  \"query_5\": \"accuracy and reliability of LLM judges in AI evaluation\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "oai_args = get_completion_args(provider=\"openai\", topic = \"Using a panel of LLM judges to evaluate the correctness of another LLM\", num_queries=5)\n",
    "oai_res = chat_completion_request(**oai_args)\n",
    "print(json.dumps(json.loads(oai_res.choices[0].message.tool_calls[0].function.arguments), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97487e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advantages of using LLM judges to evaluate other LLMs',\n",
       " 'methodologies for implementing a panel of LLM judges',\n",
       " 'case studies on LLM judges evaluating other LLMs',\n",
       " 'challenges in using LLMs to judge other LLMs',\n",
       " 'accuracy and reliability of LLM judges in AI evaluation']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = json.loads(oai_res.choices[0].message.tool_calls[0].function.arguments)\n",
    "queries = [query for _,query in queries.items()]\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b25426d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     exa_pairs\u001b[38;5;241m.\u001b[39mappend(query_object)\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m exa_pairs\n\u001b[0;32m---> 18\u001b[0m exa_pairs \u001b[38;5;241m=\u001b[39m \u001b[43msearch_exa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m exa_pairs\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36msearch_exa\u001b[0;34m(queries)\u001b[0m\n\u001b[1;32m      2\u001b[0m exa_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m----> 4\u001b[0m   search_response \u001b[38;5;241m=\u001b[39m \u001b[43mexa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_and_contents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_autoprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_published_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-06-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# To give us only recent information post-June 2023\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_sentences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   query_object \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubquery\u001b[39m\u001b[38;5;124m'\u001b[39m: query,\n\u001b[1;32m     13\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m: search_response\n\u001b[1;32m     14\u001b[0m   }\n\u001b[1;32m     15\u001b[0m   exa_pairs\u001b[38;5;241m.\u001b[39mappend(query_object)\n",
      "File \u001b[0;32m~/code/llm/llm-experiments/notebooks/.venv/lib/python3.11/site-packages/exa_py/api.py:497\u001b[0m, in \u001b[0;36mExa.search_and_contents\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m options \u001b[38;5;241m=\u001b[39m to_camel_case(options)\n\u001b[1;32m    495\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/search\u001b[39m\u001b[38;5;124m\"\u001b[39m, options)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SearchResponse(\n\u001b[0;32m--> 497\u001b[0m     [Result(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_snake_case(result)) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m],\n\u001b[1;32m    498\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautopromptString\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautopromptString\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    499\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "def search_exa(queries: list) -> list:\n",
    "  exa_pairs = []\n",
    "  for query in queries:\n",
    "    search_response = exa.search_and_contents(\n",
    "      query,\n",
    "      num_results=5,\n",
    "      use_autoprompt=True,\n",
    "      start_published_date=\"2023-06-01\", # To give us only recent information post-June 2023\n",
    "      highlights={\"num_sentences\": 5},\n",
    "    )\n",
    "    query_object = {\n",
    "        'subquery': query,\n",
    "        'results': search_response\n",
    "    }\n",
    "    exa_pairs.append(query_object)\n",
    "  return exa_pairs\n",
    "\n",
    "exa_pairs = search_exa(queries)\n",
    "exa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f2802",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2b9008e-26b4-46dc-ba3a-a6deebe21b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# <span style='color:#008080;font-family:CaskaydiaCove Nerd Font Mono'>LLM Assistant</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "When it comes to API design, there are several key tenets to keep in mind to ensure your API is scalable, maintainable, and easy to use. Here are the top ones:\n",
       "\n",
       "1. **API First Development**: Design your API before implementing it. This helps you focus on the interface and its usability, rather than just the implementation details.\n",
       "2. **Simple and Consistent**: Keep your API simple, intuitive, and consistent in its design. This makes it easier for developers to learn and use.\n",
       "3. **RESTful**: Follow REST (Representational State of Resource) principles, which emphasize stateless, cacheable, and uniform interfaces.\n",
       "4. **Resource-Based**: Organize your API around resources, which are objects or entities that can be manipulated. Use nouns to identify resources (e.g., /users).\n",
       "5. **Verb-Based**: Use standard HTTP verbs (GET, POST, PUT, DELETE) to perform actions on resources.\n",
       "6. **Stateless**: Ensure your API is stateless, meaning each request contains all the information necessary to fulfill it.\n",
       "7. **Cacheable**: Design your API to allow for caching, which reduces the load on your servers and improves performance.\n",
       "8. **Error Handling**: Implement robust error handling, including meaningful error codes, messages, and descriptions.\n",
       "9. **API Versioning**: Use versioning to manage changes to your API over time, ensuring backward compatibility and minimizing disruptions.\n",
       "10. **Documentation**: Provide comprehensive, up-to-date, and easily accessible documentation, including code samples and tutorials.\n",
       "11. **Security**: Implement robust security measures, such as authentication, authorization, and encryption, to protect your API and its users.\n",
       "12. **Performance**: Optimize your API for performance, considering factors like response time, throughput, and resource utilization.\n",
       "13. **Flexibility**: Design your API to be flexible and accommodating of different data formats, such as JSON, XML, or YAML.\n",
       "14. **API Keys and Rate Limiting**: Implement API keys and rate limiting to manage access, prevent abuse, and ensure fair usage.\n",
       "15. **Monitoring and Analytics**: Monitor your API's performance and usage, collecting metrics and analytics to inform future development and optimization.\n",
       "\n",
       "By following these key tenets, you'll be well on your way to designing a robust, scalable, and user-friendly API that meets the needs of your users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = OpenAI(api_key=GROQ_API_KEY, base_url=GROQ_BASE_URL)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=0.8,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests and being concise as possible.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the key tenets of API Design\"}\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "display(Markdown(\"# <span style='color:#008080;font-family:CaskaydiaCove Nerd Font Mono'>\" + \"LLM Assistant\" + \"</span>\"))\n",
    "display(Markdown(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1a4bb-473b-4513-9578-bcc6590b2ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
